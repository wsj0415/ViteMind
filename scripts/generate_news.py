import requests
import json
import os
from datetime import datetime
import feedparser

# é…ç½®
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL_NAME = "xiaomi/mimo-v2-flash:free" 

# æ•°æ®æº
RSS_FEEDS = [
    "https://hnrss.org/newest?q=AI", # Hacker News AI
    "https://www.theverge.com/rss/artificial-intelligence/index.xml", # The Verge AI
    "https://techcrunch.com/category/artificial-intelligence/feed/", # TechCrunch AI
    "https://huggingface.co/papers/rss", # Hugging Face Papers (Dev/Research)
    "https://machinelearningmastery.com/blog/feed/", # ML Mastery (Dev/Tutorial)
    "https://www.producthunt.com/feed?topic=artificial-intelligence", # Product Hunt AI (New Tools)
    "https://appsumo.com/feed/", # AppSumo (Deals - need filtering)
]

def fetch_rss_data():
    articles = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    for feed_url in RSS_FEEDS:
        print(f"Fetching {feed_url}...")
        try:
            response = requests.get(feed_url, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"Failed to fetch {feed_url}, status code: {response.status_code}")
                continue
                
            feed = feedparser.parse(response.content)
            
            print(f"Found {len(feed.entries)} entries in {feed_url}")
            # å¯¹äº AppSumo ç­‰é«˜é¢‘æºï¼Œæˆ–è€…æ— å…³å†…å®¹è¾ƒå¤šçš„æºï¼Œå¯ä»¥é€‚å½“å¢åŠ è·å–æ•°é‡ä»¥ä¾¿åç»­ LLM ç­›é€‰ï¼Œä½†ä¸ºäº†é€Ÿåº¦å…ˆå–å‰ 3-5 æ¡
            limit = 5
            if "appsumo" in feed_url: 
                limit = 10 # å¤šå–ä¸€äº›ä»¥ä¾¿ç­›é€‰ AI ç›¸å…³
            
            for entry in feed.entries[:limit]: 
                # ä½¿ç”¨ Jina Reader è¯»å–å…¨æ–‡
                jina_url = f"https://r.jina.ai/{entry.link}"
                print(f"Reading with Jina: {jina_url}")
                try:
                    # Jina å¯èƒ½ä¼šå¯¹æŸäº› User-Agent æ•æ„Ÿï¼Œå°è¯•ä¸å¸¦ç‰¹æ®Š UA æˆ–ä½¿ç”¨é»˜è®¤
                    jina_resp = requests.get(jina_url, timeout=30) 
                    
                    if jina_resp.status_code == 200 and "403 Forbidden" not in jina_resp.text:
                        content = jina_resp.text[:2000] # å¢åŠ æˆªå–é•¿åº¦
                    else:
                        print(f"Jina returned {jina_resp.status_code}, falling back to summary.")
                        content = entry.get("summary", "")
                except Exception as e:
                    print(f"Jina read failed: {e}")
                    content = entry.get("summary", "")

                if len(content) < 50:
                     content = entry.get("summary", "")

                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "summary": content 
                })
        except Exception as e:
            print(f"Error fetching {feed_url}: {e}")
    return articles

def summarize_with_ai(articles):
    if not articles:
        return []

    # æ„å»º Prompt
    news_text = "\n".join([f"- [{a['title']}]({a['link']}): {a['summary']}" for a in articles])
    prompt = f"""
    ä½ æ˜¯ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·é˜…è¯»ä»¥ä¸‹åŸå§‹æ–°é—»åˆ—è¡¨ï¼Œç­›é€‰å‡º 8-12 æ¡æœ€æœ‰ä»·å€¼çš„å†…å®¹ã€‚
    
    **ç­›é€‰æ ‡å‡†ï¼ˆå¿…é¡»è¦†ç›–ä»¥ä¸‹ç±»åˆ«ï¼‰ï¼š**
    1. ğŸš¨ **å¤§äº‹ä»¶ (News)**: 24å°æ—¶å†…çš„é‡å¤§ AI æ–°é—» (OpenAI, Google ç­‰)ã€‚
    2. ğŸ **ä¿ƒé”€ (Deals)**: AI äº§å“çš„é™æ—¶ä¼˜æƒ ã€Lifetime Deal (å¦‚ AppSumo ä¸Šçš„ AI å·¥å…·)ã€‚
    3. ğŸ› ï¸ **ç¼–ç¨‹ (Dev)**: AI å¼€å‘æ•™ç¨‹ã€Hugging Face è®ºæ–‡ã€LLM éƒ¨ç½²æŒ‡å—ã€‚
    4. ğŸš€ **æ–°äº§å“ (New)**: Product Hunt ä¸Šçš„çƒ­é—¨ AI æ–°å“ (ç±»ä¼¼ TAAFT æ—¶é—´è½´)ã€‚

    è¯·è¾“å‡ºä¸€ä¸ªçº¯ JSON æ•°ç»„ï¼ˆä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®° ```json ... ```ï¼‰ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - title: (string) ä¸­æ–‡æ ‡é¢˜ï¼Œå¸å¼•äººä¸”ä¸“ä¸šã€‚
    - summary: (string) ä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼Œç”¨äºå¡ç‰‡å±•ç¤ºã€‚
    - detail: (string) è¯¦ç»†çš„ä¸­æ–‡æ·±åº¦è§£è¯»ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼ŒåŒ…å«èƒŒæ™¯ã€æ ¸å¿ƒæŠ€æœ¯ç‚¹ã€è¡Œä¸šå½±å“ç­‰ï¼ˆ300å­—å·¦å³ï¼‰ã€‚
    - tags: (array of strings) å¿…é¡»åŒ…å«ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ ["News", "Deal", "Dev", "New"]ï¼Œä»¥åŠ 1-2 ä¸ªå†…å®¹æ ‡ç­¾ (å¦‚ "LLM", "Python")ã€‚
    - link: (string) åŸå§‹é“¾æ¥ã€‚
    - date: (string) æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDã€‚

    åŸå§‹æ–°é—»ï¼š
    {news_text}
    """

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://vitemind.com", 
    }
    
    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}]
    }

    try:
        response = requests.post(OPENROUTER_URL, headers=headers, json=data)
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content']
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®°
        content = content.replace("```json", "").replace("```", "").strip()
        
        return json.loads(content)
    except Exception as e:
        print(f"AI Generation Error: {e}")
        return []

def save_to_json(new_items):
    if not new_items:
        print("No new items to save.")
        return

    file_path = "docs/public/data/news.json"
    
    # è¯»å–ç°æœ‰æ•°æ®
    existing_data = []
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        except json.JSONDecodeError:
            existing_data = []
    
    # åˆå¹¶æ•°æ®ï¼ˆå°†æ–°æ•°æ®æ’åˆ°æœ€å‰é¢ï¼‰
    # ä¸ºæ¯æ¡æ•°æ®æ·»åŠ  ID (ç®€å•çš„åŸºäºæ—¶é—´æˆ³)
    for item in new_items:
        item['id'] = str(int(datetime.now().timestamp() * 1000)) + str(new_items.index(item))
        # ç¡®ä¿æ—¥æœŸå­—æ®µå­˜åœ¨
        if 'date' not in item:
            item['date'] = datetime.now().strftime("%Y-%m-%d")

    updated_data = new_items + existing_data
    
    # é™åˆ¶æ€»æ¡æ•°ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§ï¼ˆä¿ç•™æœ€è¿‘ 100 æ¡ï¼‰
    updated_data = updated_data[:100]

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(updated_data, f, ensure_ascii=False, indent=2)
    print(f"Saved {len(new_items)} items to {file_path}")

def save_to_supabase(new_items):
    try:
        from supabase import create_client, Client
    except ImportError:
        print("Supabase library not installed. Skipping Supabase upload.")
        return

    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")

    if not url or not key:
        print("Supabase credentials not found. Skipping Supabase upload.")
        return

    try:
        supabase: Client = create_client(url, key)
        
        # å‡†å¤‡æ•°æ®ï¼Œç¡®ä¿æ ¼å¼ç¬¦åˆ Supabase è¡¨ç»“æ„
        data_to_upsert = []
        for item in new_items:
            data_to_upsert.append({
                "id": item.get("id"),
                "title": item.get("title"),
                "summary": item.get("summary"),
                "detail": item.get("detail"),
                "link": item.get("link"),
                "tags": item.get("tags"),
                "date": item.get("date")
            })
            
        if not data_to_upsert:
            return

        # æ‰§è¡Œ Upsert
        response = supabase.table("news").upsert(data_to_upsert).execute()
        print(f"Successfully uploaded {len(data_to_upsert)} items to Supabase.")
        
    except Exception as e:
        print(f"Error uploading to Supabase: {e}")

if __name__ == "__main__":
    if not OPENROUTER_API_KEY:
        print("Error: OPENROUTER_API_KEY not found.")
        exit(1)

    print("Starting AI News Generator (JSON + Supabase)...")
    raw_articles = fetch_rss_data()
    print(f"Fetched {len(raw_articles)} articles.")
    
    ai_json = summarize_with_ai(raw_articles)
    
    # 1. ä¿å­˜ä¸ºæœ¬åœ° JSON (ç”¨äºå‰ç«¯é™æ€å±•ç¤º)
    save_to_json(ai_json)
    
    # 2. ä¸Šä¼ åˆ° Supabase (ç”¨äºæ•°æ®å½’æ¡£)
    save_to_supabase(ai_json)
    
    print("Done.")
